{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install --upgrade pip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install dabl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import dabl\nfrom pandas_profiling import ProfileReport\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, classification_report\n\nimport gc #python garbage collection\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', 300)\npd.set_option('display.max_rows',200)\npd.set_option('display.width', 1000)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', '{:20,.2f}'.format)\nnp.random.seed(123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_COL = 'diabetes_mellitus'\ndf = pd.read_csv(\"/kaggle/input/widsdatathon2021/TrainingWiDS2021.csv\")\ntest = pd.read_csv(\"/kaggle/input/widsdatathon2021/UnlabeledWiDS2021.csv\")\n\nprint('The size of training set is: ', df.shape)\nprint('The size of the test set is: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full = pd.read_csv(\"/kaggle/input/widsdatathon2021/UnlabeledWiDS2021.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get data info\ndf[TARGET_COL].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get infomation about numeric data\ndf.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#quick data profiling\n#train_profile = ProfileReport(df, 'EDA')\n#train_profile\n#output the profiling file\n#train_profile.to_file(\"output.html\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nchoose collumn to drop before the importance selection based on data nature\nreadmission_status:unique value to drop\nhospital_id\nicu_id\n\n\n\"\"\"\ncols_drop = ['readmission_status', 'hospital_id', 'icu_id','Unnamed: 0']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop feature value is id or unique\ndf.drop(cols_drop, axis =1, inplace = True)\ntest.drop(cols_drop, axis =1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Find feature importance <br>\nBased on data profiling,we notice there are many missing values in mutiple columns, so will use the randomforest to decide the feature importance.<br>\nThen drop the values and impute values."},{"metadata":{"trusted":true},"cell_type":"code","source":"import h2o\nfrom h2o.estimators import H2ORandomForestEstimator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o.init()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o_df = h2o.H2OFrame(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#h2o_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define model\nmodel = H2ORandomForestEstimator(ntrees=50, max_depth=20, nfolds=10)\n\n# Train model\nmodel.train(y=TARGET_COL, training_frame=h2o_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get feature importance\nimportance_df = model.varimp(use_pandas=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read the feature importance table\nimportance_df = pd.read_csv('/kaggle/input/featureimportance/DataDictionaryWiDS2021.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_to_drop = importance_df[importance_df['percentage']==0]['relative_importance']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop features which the importance score is 0\ndf.drop(feature_to_drop , axis =1, inplace = True)\ntest.drop(feature_to_drop, axis =1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop target variable\nX_train = df.drop(TARGET_COL, axis = 1)\ny_train = df[TARGET_COL].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Check missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#find columns have missing value after drop those none important columns\nmissing_pct = X_train.isna().mean()\nmissing_pct  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get list of column name which missing pct>25%\nmissing_pct.loc[missing_pct >=0.25]\nmissing_cols = ((missing_pct.loc[missing_pct >=0.25]).index).to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get categorical columns\nmissing_cat = [c for c in missing_cols if (X_train[c].nunique()>1) &\n              (X_train[c].dtype != np.number) & (X_train[c].dtype !=int)]\nmissing_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def categorical_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', verbose=True):\n    '''\n    Helper function that gives a quick summary of a given column of categorical data\n    Arguments\n    =========\n    dataframe: pandas dataframe\n    x: str. horizontal axis to plot the labels of categorical data, y would be the count\n    y: str. vertical axis to plot the labels of categorical data, x would be the count\n    hue: str. if you want to compare it another variable (usually the target variable)\n    palette: array-like. Colour of the plot\n    Returns\n    =======\n    Quick Stats of the data and also the count plot\n    '''\n    if x == None:\n        column_interested = y\n    else:\n        column_interested = x\n    series = dataframe[column_interested]\n    print(series.describe())\n    print('mode: ', series.mode())\n    if verbose:\n        print('='*80)\n        print(series.value_counts())\n\n    sns.countplot(x=x, y=y, hue=hue, data=dataframe, palette=palette)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_num =  [c for c in missing_cols if (X_train[c].nunique()>1) &\n              (X_train[c].dtype == np.number)]\nmissing_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing_num.append(TARGET_COL)\n#missing_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing_corr = df[missing_num].corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for some features, even the corr is 0, but feature importance is high, so can't use corr\n#missing_corr[TARGET_COL].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(missing_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', size = 15)\ncolormap = sns.diverging_palette(10, 220, as_cmap = True)\nsns.heatmap(missing_corr,\n            cmap = colormap,\n            square = True,\n            annot = True,\n            linewidths=0.1,vmax=1.0, linecolor='white',\n            annot_kws={'fontsize':12 })\nplt.show()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Missing Data Imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#MICE imputation\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.imputation import mice\nfrom statsmodels.imputation.bayes_mi import BayesGaussMI, MI","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/residentmario/simple-techniques-for-missing-data-imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#imp_data = mice.MICEData(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://github.com/shineloveyc/fancyimpute"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Matrix completion by iterative soft thresholding of SVD decompositions. \nInspired by the softImpute package for R, which is based on Spectral \nRegularization Algorithms for Learning Large Incomplete Matrices by Mazumder et.\nal.\"\"\"\nfrom fancyimpute import SoftImpute, BiScaler,IterativeImputer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_normalized = BiScaler().fit_transform(X_train.values)\n#X_train_complete_soft = SoftImpute().fit_transform(X_train_normalized)\nX_train_complete_mice = IterativeImputer().fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#impute test set\n#X_test_normalized = BiScaler().fit_transform(test.values)\n#X_test_complete_soft = SoftImpute().fit_transform(X_test_normalized)\nX_test_complete_mice = IterativeImputer().fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_df = pd.DataFrame(X_test_complete_mice, columns = test.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* build the base line model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import h2o\nfrom h2o.automl import H2OAutoML","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(X_train_complete_mice)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_h2o = pd.DataFrame(X_train_complete_mice, columns = X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_h2o.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_h2o.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check missing value==>all imputed\ntrain_df_h2o.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combine preditive data with target data and fit into h2o dataframe\ntrain_df_h2o[TARGET_COL] = y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o_train_df = h2o.H2OFrame(train_df_h2o)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o_train_df[TARGET_COL] = h2o_train_df[TARGET_COL].asfactor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\naml = H2OAutoML(max_models=20, seed=1)\naml.train(y=TARGET_COL, training_frame=h2o_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* save model"},{"metadata":{},"cell_type":"markdown","source":"There are two ways to save the leader model -- binary format and MOJO format. If you're taking your leader model to production, then we'd suggest the MOJO format since it's optimized for production use."},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o.save_model(aml.leader, path = \"./product_backorders_model_bin\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aml.leader.download_mojo(path = \"./\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*training the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get model ids for all models in the AutoML Leaderboard\nmodel_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n# Get the \"All Models\" Stacked Ensemble model\nse = h2o.get_model([mid for mid in model_ids if \"StackedEnsemble_AllModels\" in mid][0])\n# Get the Stacked Ensemble metalearner model\nmetalearner = h2o.get_model(se.metalearner()['name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o_test_df = h2o.H2OFrame(X_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = se.predict(h2o_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_df = h2o.as_list(predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_df['encounter_id'] = test_full['encounter_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_df.rename(columns = {'p1':'diabetes_mellitus'}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_df[['encounter_id', 'diabetes_mellitus']].to_csv('/kaggle/working/submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}