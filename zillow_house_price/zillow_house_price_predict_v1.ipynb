{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "zillow_house_price_predict-v1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shineloveyc/Kaggle-Competition/blob/main/zillow_house_price_predict_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUc7dAAoWQiA"
      },
      "source": [
        "## Project Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2gnasBeWQiC"
      },
      "source": [
        "In this competition, Zillow is asking you to predict the log-error between their Zestimate and the actual sale price, given all the features of a home. The log error is defined as\n",
        "\n",
        "                                 logerror=log(Zestimate)âˆ’log(SalePrice)\n",
        "and it is recorded in the transactions file train.csv. In this competition, you are going to predict the logerror for the months in Fall 2017. Since all the real estate transactions in the U.S. are publicly available, we will close the competition (no longer accepting submissions) before the evaluation period begins.\n",
        "\n",
        "* Train/Test split\n",
        "You are provided with a full list of real estate properties in three counties (Los Angeles, Orange and Ventura, California) data in 2016.\n",
        "* The train data has all the transactions before October 15, 2016, plus some of the transactions after October 15, 2016.\n",
        "* The test data in the public leaderboard has the rest of the transactions between October 15 and December 31, 2016.\n",
        "* The rest of the test data, which is used for calculating the private leaderboard, is all the properties in October 15, 2017, to December 15, 2017. This period is called the \"sales tracking period\", during which we will not be taking any submissions.\n",
        "* You are asked to predict 6 time points for all properties: October 2016 (201610), November 2016 (201611), December 2016 (201612), October 2017 (201710), November 2017 (201711), and December 2017 (201712).\n",
        "* Not all the properties are sold in each time period. If a property was not sold in a certain time period, that particular row will be ignored when calculating your score.\n",
        "* If a property is sold multiple times within 31 days, we take the first reasonable value as the ground truth. By \"reasonable\", we mean if the data seems wrong, we will take the transaction that has a value that makes more sense.\n",
        "* File descriptions\n",
        "* properties_2016.csv - all the properties with their home features for 2016. Note: Some 2017 new properties don't have any data yet except for their parcelid's. Those data points should be populated when properties_2017.csv is available.\n",
        "* properties_2017.csv - all the properties with their home features for 2017 (released on 10/2/2017)\n",
        "* train_2016.csv - the training set with transactions from 1/1/2016 to 12/31/2016\n",
        "* train_2017.csv - the training set with transactions from 1/1/2017 to 9/15/2017 (released on 10/2/2017)\n",
        "sample_submission.csv - a sample submission file in the correct format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "trMymyMKWQiD"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRssiCNXWQiG"
      },
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "59EohzOZWQiH"
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import missingno as msno\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from scipy.stats import ttest_ind\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.feature_selection import SelectFromModel\n",
        "import h2o\n",
        "from h2o.estimators import H2ORandomForestEstimator\n",
        "from h2o.grid.grid_search import H2OGridSearch\n",
        "\n",
        "#sets up pandas table display\n",
        "pd.set_option('display.width', 800)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.notebook_repr_html', True)\n",
        "#stop scientific notation\n",
        "pd.options.display.float_format = '{:.2f}'.format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0EODiNZpWQiJ"
      },
      "source": [
        "# Making a list of missing value types\n",
        "missing_values = [\"n/a\", \"na\", \"--\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TEQuoxO9WQiL"
      },
      "source": [
        "#load the 2016 properties data and target variable\n",
        "house_2016_df = pd.read_csv('../input/properties_2016.csv', na_values = missing_values, low_memory=False)\n",
        "house_2017_df = pd.read_csv('../input/properties_2017.csv', na_values = missing_values, low_memory=False)\n",
        "house_log_2016 = pd.read_csv('../input/train_2016_v2.csv', low_memory=False)\n",
        "house_log_2017 = pd.read_csv('../input/train_2017.csv', low_memory=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fh1ksUuhWQiN"
      },
      "source": [
        "#get the size\n",
        "print(house_2016_df.shape)\n",
        "print(house_2017_df.shape)\n",
        "print(house_log_2016.shape)\n",
        "print(house_log_2017.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BqJCAfyeWQiP"
      },
      "source": [
        "house_2017_df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mJqxLY-RWQiR"
      },
      "source": [
        "#merge the trasaction dataset fro 2016-2017\n",
        "house_log_full = pd.concat([house_log_2016,house_log_2017],ignore_index=True)\n",
        "house_log_full.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "D6ZEJSipWQiV"
      },
      "source": [
        "#join the 2017 train set with 2016&2017 target variable\n",
        "house_2017_full = house_2017_df.merge(house_log_full, on = 'parcelid')\n",
        "house_2017_full.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UqDVPztWQiY"
      },
      "source": [
        "## Data EDA\n",
        "Will use 2017 properties data as it is a more completed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VwVGlnlsWQia"
      },
      "source": [
        "#check the size after join\n",
        "house_2017_full.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YazVK0kSWQid"
      },
      "source": [
        "#impute boolean variables \n",
        "house_2017_full['fireplaceflag'].replace(True, 1, inplace=True)\n",
        "house_2017_full['fireplaceflag'].fillna(0, inplace = True)\n",
        "house_2017_full['hashottuborspa'].replace(True, 1, inplace=True)\n",
        "house_2017_full['hashottuborspa'].fillna(0, inplace = True) \n",
        "house_2017_full['pooltypeid10'].fillna(0, inplace = True) \n",
        "house_2017_full['pooltypeid2'].fillna(0, inplace = True)\n",
        "house_2017_full['pooltypeid7'].fillna(0, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sQTTgXFcWQif"
      },
      "source": [
        "#plot distribution of target variable log error\n",
        "from scipy.stats import zscore\n",
        "house_2017_full[\"logerror_zscore\"] = zscore(house_2017_full[\"logerror\"])\n",
        "house_2017_full[\"is_outlier\"] = house_2017_full[\"logerror_zscore\"].apply(\n",
        "  lambda x: x <= -2.5 or x >= 2.5\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sb.distplot(house_2017_full[~house_2017_full['is_outlier']].logerror.values, bins=50, kde=False)\n",
        "plt.xlabel('logerror', fontsize=12)\n",
        "plt.title('logerror distribution')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AdfKDrq7WQii"
      },
      "source": [
        "#explore the missing value\n",
        "missing_df = house_2017_full.isnull().sum(axis=0).reset_index()\n",
        "missing_df.columns = ['column_name', 'missing_count']\n",
        "missing_df = missing_df.loc[missing_df['missing_count']>0]\n",
        "missing_df = missing_df.sort_values(by='missing_count')\n",
        "\n",
        "ind = np.arange(missing_df.shape[0])\n",
        "width = 0.9\n",
        "fig, ax = plt.subplots(figsize=(12,18))\n",
        "rects = ax.barh(ind, missing_df.missing_count.values, color='lightblue')\n",
        "ax.set_yticks(ind)\n",
        "ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n",
        "ax.set_xlabel(\"Count of missing values\")\n",
        "ax.set_title(\"Number of missing values in each column\")\n",
        "plt.show()                    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ckwzTEQUWQik"
      },
      "source": [
        "#for continues variables, plot the scatter plot\n",
        "low_na_num_features = ['lotsizesquarefeet','finishedsquarefeet12','calculatedbathnbr','fullbathcnt','roomcnt','bedroomcnt','bathroomcnt','landtaxvaluedollarcnt', 'taxamount', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "o2mEtYq9WQim"
      },
      "source": [
        "house_2017_full[low_na_num_features].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wDVIQTYxWQio"
      },
      "source": [
        "#explore categorical features with low missing values\n",
        "low_na_cat_features = ['airconditioningtypeid', 'decktypeid', 'heatingorsystemtypeid', \n",
        "                       'hashottuborspa', 'heatingorsystemtypeid','regionidcounty', 'regionidcity',\n",
        "                       'propertycountylandusecode','propertylandusetypeid','yearbuilt','assessmentyear']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "No-p2kXDWQiq"
      },
      "source": [
        "#correlation matrix to measure the corr between continuous variables\n",
        "num_var = ['basementsqft', 'bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid','buildingclasstypeid','threequarterbathnbr','calculatedfinishedsquarefeet',\n",
        "           'calculatedbathnbr','finishedsquarefeet6','finishedsquarefeet12', 'finishedsquarefeet13', 'finishedsquarefeet15','finishedsquarefeet50','fireplacecnt',\n",
        "           'fullbathcnt','garagecarcnt', 'garagetotalsqft','lotsizesquarefeet','poolsizesum', 'poolcnt', 'numberofstories', 'poolcnt', 'poolsizesum','roomcnt', \n",
        "           'unitcnt','yardbuildingsqft17','yearbuilt', 'yardbuildingsqft26', 'latitude', 'longitude','taxvaluedollarcnt', 'structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', \n",
        "           'taxamount','logerror']\n",
        "corr = house_2017_full[num_var].corr()\n",
        "ig, ax = plt.subplots(figsize=(25,15)) \n",
        "sb.heatmap(corr, cmap=\"YlGnBu\", annot=True, ax = ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lgLi5FZLWQis"
      },
      "source": [
        "# Drop Multicollinearity variables\n",
        "house_2017_full.drop(['taxvaluedollarcnt', 'structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'bathroomcnt',\n",
        "                      'fullbathcnt','finishedsquarefeet6', 'finishedsquarefeet12', 'finishedsquarefeet13','finishedsquarefeet15',\n",
        "                      'finishedsquarefeet50'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "m_63SkUzWQit"
      },
      "source": [
        "#one way ANOVA analysis between cat variables has more than two levels and logerror\n",
        "for i in range(0, len(low_na_cat_features)):\n",
        "    formular = str('logerror ~ '+low_na_cat_features[i])\n",
        "    model = ols(formular,data = house_2017_full).fit()\n",
        "    anova_result = sm.stats.anova_lm(model, typ=2)\n",
        "    print (anova_result)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBJThLxyWQiw"
      },
      "source": [
        "* significant var:'airconditioningtypeid', 'decktypeid', 'heatingorsystemtypeid', 'hashottuborspa', 'heatingorsystemtypeid','regionidcounty', 'regionidcity', 'regionidneighborhood', 'storytypeid', 'typeconstructiontypeid', 'assessmentyear','yearbuilt'\n",
        "* not significant var: 'architecturalstyletypeid','propertylandusetypeid'regionidzip'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "igXSJnMKWQix"
      },
      "source": [
        "#t test between cat variables has two levels and taxamount\n",
        "ttest_ind(house_2017_full['logerror'], house_2017_full['fireplaceflag'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deU9PDRGWQi1"
      },
      "source": [
        "Feature selection by using H2O Random Forest <br>\n",
        "H2O intro: http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/intro.html <br>\n",
        "http://docs.h2o.ai/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pcoQjLiAWQi2"
      },
      "source": [
        "# since the dataset has many categorical variables, and using sklean random forest requires one-hot encoding\n",
        "# so we use h2o random forest instead\n",
        "# create h2o object\n",
        "h2o.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DESOin3RWQi5"
      },
      "source": [
        "#convert the pandas df to h2o frame\n",
        "house_2017_tree = house_2017_full.drop(['logerror_zscore','is_outlier','parcelid'], axis =1)\n",
        "# house_2017_full_hf = h2o.H2OFrame(house_2017_tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Kl1mdQckWQi9"
      },
      "source": [
        "#defind the model\n",
        "# h2o_tree = H2ORandomForestEstimator(ntrees = 50, max_depth = 20, nfolds =10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kDt5aayDWQi-"
      },
      "source": [
        "#train the model,if x not specify,model will use all x except the y column\n",
        "# h2o_tree.train(y = 'logerror', training_frame = house_2017_full_hf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hw0rms_WQjA"
      },
      "source": [
        "https://aichamp.wordpress.com/2017/04/11/working-with-variable-importance-data-with-models-in-h2o/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Xh8PWeiyWQjB"
      },
      "source": [
        "#print variable importance\n",
        "# h2o_tree_df = h2o_tree._model_json['output']['variable_importances'].as_data_frame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LPG43A3uWQjF"
      },
      "source": [
        "#visualize the importance\n",
        "\"\"\"\n",
        "plt.rcdefaults()\n",
        "fig, ax = plt.subplots(figsize = (10, 10))\n",
        "variables = h2o_tree._model_json['output']['variable_importances']['variable']\n",
        "y_pos = np.arange(len(variables))\n",
        "scaled_importance = h2o_tree._model_json['output']['variable_importances']['scaled_importance']\n",
        "ax.barh(y_pos, scaled_importance, align='center', color='green', ecolor='black')\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(variables)\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('Scaled Importance')\n",
        "ax.set_title('Variable Importance')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "URVUTZpPWQjI"
      },
      "source": [
        "#choose features have importance score >0.2\n",
        "\"\"\"\n",
        "feature_score = 0.2\n",
        "selected_features = h2o_tree_df[h2o_tree_df.scaled_importance>=feature_score]['variable']\n",
        "selected_features\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hHH3t1RvWQjK"
      },
      "source": [
        "selected_features = ['transactiondate', 'regionidneighborhood','taxamount','calculatedfinishedsquarefeet'\n",
        "                     ,'yearbuilt','lotsizesquarefeet','propertyzoningdesc','garagetotalsqft'\n",
        "                     ,'latitude','longitude','bedroomcnt','buildingqualitytypeid'\n",
        "                     ,'calculatedbathnbr','yardbuildingsqft17']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BidYeJ7MWQjM"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnEPFsO0WQjN"
      },
      "source": [
        "H2O: Missing values are interpreted as containing information (i.e., missing for a reason), rather than missing at random. During tree building, split decisions for every node are found by minimizing the loss function and treating missing values as a separate category that can go either left or right. XGBoost will automatically learn which is the best direction to go when a value is missing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wiy7PhOCWQjO"
      },
      "source": [
        "## Build XGboosting model by using h2o\n",
        "http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html<br>\n",
        "http://uc-r.github.io/regression_preparation<br>\n",
        "https://dzone.com/articles/machine-learning-with-h2o-hands-on-guide-for-data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4Q8Ip0P4WQjP"
      },
      "source": [
        "#create h2o instance\n",
        "# h2o.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jc3mOD9NWQjR"
      },
      "source": [
        "selected_cols = (pd.Series(selected_features)).append(pd.Series(['logerror']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CQpK7jGbWQjT"
      },
      "source": [
        "#split data to training and test data set\n",
        "X_train,X_test= train_test_split(house_2017_tree[selected_cols], test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tY8RJ9J-WQjW"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oGbtMp7mWQjY"
      },
      "source": [
        "X_train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ETpvKFiVWQja"
      },
      "source": [
        "#transfer to h2o dataframe\n",
        "X_train_h2o = h2o.H2OFrame(X_train)\n",
        "X_test_h2o = h2o.H2OFrame(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VGFM14jrWQjc"
      },
      "source": [
        "param = {\n",
        "      \"ntrees\" : 100\n",
        "    , \"learn_rate\" : 0.02\n",
        "    , \"max_depth\" : 10\n",
        "    , \"max_depth\" : 10\n",
        "    , \"sample_rate\" : 0.7\n",
        "    , \"col_sample_rate_per_tree\" : 0.9\n",
        "    , \"min_rows\" : 5\n",
        "    , \"seed\": 4241\n",
        "    , \"score_tree_interval\": 100\n",
        "    ,  'nfolds': 10\n",
        "    , \"stopping_metric\" : \"MSE\"\n",
        "}\n",
        "from h2o.estimators import H2OXGBoostEstimator\n",
        "model = H2OXGBoostEstimator(**param)\n",
        "model.train(y = 'logerror', training_frame = X_train_h2o)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejy9J80IWQje"
      },
      "source": [
        "model evaluation: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html#metric-best-practices-regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AzwKlBW1WQjf"
      },
      "source": [
        "#print training model summary\n",
        "print(model.summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CXHmYMVSWQjh"
      },
      "source": [
        "my_metrics = model.model_performance(test_data=X_test_h2o) # create the new test set metrics\n",
        "my_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQSePs4OWQjl"
      },
      "source": [
        "### Hyper-Parameter Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fratT8sEWQjm"
      },
      "source": [
        "hyper_params = {'max_depth' : [4,6,8,12,16,20]\n",
        "               ,\"learn_rate\" : [0.1, 0.01, 0.0001]\n",
        "               }\n",
        "param_grid = {\n",
        "      \"ntrees\" : 100\n",
        "    , \"max_depth\" : 10\n",
        "    , \"sample_rate\" : 0.7\n",
        "    , \"col_sample_rate_per_tree\" : 0.9\n",
        "    , \"min_rows\" : 5\n",
        "    , \"seed\": 4241\n",
        "    , \"score_tree_interval\": 100\n",
        "    ,  'nfolds': 10\n",
        "    , \"stopping_metric\" : \"MSE\"\n",
        "}\n",
        "model_grid = H2OXGBoostEstimator(**param_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_PbFSVbwWQjp"
      },
      "source": [
        "#Build grid search with previously made GBM and hyper parameters\n",
        "grid = H2OGridSearch(model_grid,hyper_params,\n",
        "                         grid_id = 'depth_grid',\n",
        "                         search_criteria = {'strategy': \"Cartesian\"})\n",
        "\n",
        "\n",
        "#Train grid search\n",
        "grid.train(y='logerror',\n",
        "           training_frame = X_train_h2o)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
